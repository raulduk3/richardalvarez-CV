\documentclass[a4paper,10pt]{letter}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{fontspec}

\setmainfont{Avenir}
\frenchspacing{}

% Adjust the page margins
\geometry{top=1.0in, bottom=1.0in, left=1.0in, right=1.0in}

\begin{document}

\begin{letter}{AMA Hiring Team \\
NASA Langley Research Center \\
Hampton, VA}

\opening{Dear AMA Hiring Team,}

When I first became interested in machine learning as a junior in high school, it was the ability of neural networks to find structure in unstructured data that drew me in. I was fascinated by the idea that a model could learn to recognize patterns in images, text, or sound with no explicit programming. As I followed advancements in AI, I saw firsthand how models like GPT-3.5 transformed what was possible. I started working with large language models soon after, experimenting with fine-tuning techniques and exploring how retrieval-augmented generation (RAG) could improve model accuracy. Since then, I have worked to deepen my understanding of machine learning, balancing formal coursework with independent research and development. My curiosity has driven me to read dense technical documentation, break down complex systems, and build my own applications. This process—of learning through doing, through experimentation and iteration—has shaped how I approach AI development today.

At Kenyon College, I applied this mindset to my coursework and research. I studied data structures, software development, and artificial intelligence while conducting independent projects that merged machine learning with creative industries. One of my key projects was a retrieval-augmented film recommendation system that dynamically generated API calls based on user input. To build it, I developed a pipeline that injected structured API documentation into language model queries, improving response accuracy and adaptability. This project gave me direct experience working with embeddings, vector search, and query parsing, all of which are essential to developing robust RAG architectures. Another research project involved analyzing video editing patterns using unsupervised deep learning and PySceneDetect, reinforcing my ability to process large datasets and extract meaningful insights. In both cases, my ability to dissect technical documentation and experiment with different implementations played a crucial role in solving problems efficiently.

Beyond my research, I have experience developing and deploying software in production environments. I have fine-tuned LoRA models on Linux and macOS using PyTorch, worked with various embeddings and tokenization strategies, and written efficient data processing pipelines. My background in C++ and Python has given me the flexibility to work across different computing environments, and I am comfortable debugging issues in both high- and low-level systems. More importantly, I have learned how to navigate complex codebases, parse API documentation, and determine the most effective approach to implementing a solution. Whether adapting an existing model or building a new system from scratch, I am always eager to explore different methodologies and refine my understanding.

I am excited about the opportunity to contribute to the ANOPP2 project at NASA Langley Research Center. The challenge of training a LoRA model and developing a RAG system for a large-scale aerospace application is exactly the kind of problem that motivates me. I look forward to applying my experience with documentation parsing, fine-tuning, and retrieval systems while continuing to expand my skill set. More than anything, I value the opportunity to work alongside experts who are solving real-world problems with AI. I know that I still have much to learn, and I am eager to take on that challenge.

Thank you for your time and consideration. I welcome the opportunity to discuss how my background and skills align with this role.

Sincerely, \\
Richard Alvarez \\
rawalvarez731@gmail.com \\
Chicago, Illinois

\end{letter}

\end{document}